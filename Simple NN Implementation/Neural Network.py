# -*- coding: utf-8 -*-
"""ACML NN Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qdoR-co9Dt2QaPFf-i8UDrVSWWnJ_Bz
"""

import numpy as np

class NeuralNetwork:
    def __init__(self):
        self.input_nodes = 4
        self.hidden_nodes = 8
        self.output_nodes = 3

        # Here, we initialise the weights and biases
        self.weights_input_hidden = np.ones((self.input_nodes, self.hidden_nodes))
        self.weights_hidden_output = np.ones((self.hidden_nodes, self.output_nodes))
        self.bias_hidden = np.ones(self.hidden_nodes)
        self.bias_output = np.ones(self.output_nodes)

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def feedforward(self, inputs):
        # Compute input to hidden layer
        hidden_inputs = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden
        hidden_outputs = self.sigmoid(hidden_inputs)

        # Compute hidden to output layer
        final_inputs = np.dot(hidden_outputs, self.weights_hidden_output) + self.bias_output
        final_outputs = self.sigmoid(final_inputs)

        return final_outputs

    def sum_of_squares_loss(self, outputs, targets):
        return 0.5 * np.sum((outputs - targets) ** 2)

    def backpropagation(self, inputs, targets, learning_rate):
        # Forward pass
        hidden_inputs = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden
        hidden_outputs = self.sigmoid(hidden_inputs)
        final_inputs = np.dot(hidden_outputs, self.weights_hidden_output) + self.bias_output
        final_outputs = self.sigmoid(final_inputs)

        # Backward pass
        output_errors = final_outputs - targets
        output_delta = output_errors * final_outputs * (1 - final_outputs)

        hidden_errors = np.dot(output_delta, self.weights_hidden_output.T)
        hidden_delta = hidden_errors * hidden_outputs * (1 - hidden_outputs)

        # Update weights and biases
        self.weights_hidden_output -= learning_rate * np.dot(hidden_outputs.T, output_delta)
        self.weights_input_hidden -= learning_rate * np.dot(inputs.T, hidden_delta)
        self.bias_output -= learning_rate * np.sum(output_delta, axis=0)
        self.bias_hidden -= learning_rate * np.sum(hidden_delta, axis=0)

input_values = []
for _ in range(7):
    input_values.append(float(input()))

inputs = np.array([input_values[:4]])
targets = np.array([input_values[4:]])

nn = NeuralNetwork()

initial_outputs = nn.feedforward(inputs)
initial_loss = nn.sum_of_squares_loss(initial_outputs, targets)

# Here is where the iteration of backpropagation takes place
nn.backpropagation(inputs, targets, learning_rate=0.1)

new_outputs = nn.feedforward(inputs)
new_loss = nn.sum_of_squares_loss(new_outputs, targets)

# Output the initial and new loss values rounded to 4 decimal places
print(f"{initial_loss:.4f}")
print(f"{new_loss:.4f}")